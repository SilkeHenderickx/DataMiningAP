{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import folium\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='silkeh', api_key='GwBAmaelgKfgkrmuL691')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StructType can not accept object 'tt0000001\\tshort\\tCarmencita\\tCarmencita\\t0\\t1894\\t\\\\N\\t1\\tDocumentary,Short' in type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2b8c822c0e20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#create df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtitleBasics_df\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitleBasics_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtbSchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitleBasics_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DevTools\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \"\"\"\n\u001b[1;32m--> 302\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DevTools\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DevTools\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[1;31m# make sure data could consumed multiple times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DevTools\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                 \u001b[0mverify_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DevTools\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36mverify\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1419\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m             \u001b[0mverify_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DevTools\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1407\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m                 raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\n\u001b[1;32m-> 1409\u001b[1;33m                                         % (obj, type(obj))))\n\u001b[0m\u001b[0;32m   1410\u001b[0m         \u001b[0mverify_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverify_struct\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: StructType can not accept object 'tt0000001\\tshort\\tCarmencita\\tCarmencita\\t0\\t1894\\t\\\\N\\t1\\tDocumentary,Short' in type <class 'str'>"
     ]
    }
   ],
   "source": [
    "#IMDB\n",
    "#title.basics\n",
    "#tconst, titleType, primaryTitle, originalTitle, isAdult, startYear, endYear, runtimeMinutes, genres\n",
    "titleBasics_data = (sc.textFile('D:/Workspace/_DataMining/DataSets/IMDB/titleBasics.tsv'))\n",
    "titleBasics_rdd = titleBasics_data.filter(lambda l: 'primaryTitle' not in l)\n",
    "                   \n",
    "#create schema for df\n",
    "#tbFields = titleBasicsFields\n",
    "tbFields = []\n",
    "tbFields.append(StructField('tconst', StringType(), True))\n",
    "tbFields.append(StructField('titleType', StringType(), True))\n",
    "tbFields.append(StructField('primaryTitle', StringType(), True))\n",
    "tbFields.append(StructField('originalTitle', StringType(), True))\n",
    "tbFields.append(StructField('isAdult', BooleanType(), True))\n",
    "tbFields.append(StructField('startYear', IntegerType(), True))\n",
    "tbFields.append(StructField('endYear', IntegerType(), True))\n",
    "tbFields.append(StructField('runtimeMinutes', IntegerType(), True))\n",
    "tbFields.append(StructField('genres', StringType(), True))                           \n",
    "\n",
    "tbSchema = StructType(tbFields)\n",
    "\n",
    "#create df\n",
    "titleBasics_df= sqlContext.createDataFrame(titleBasics_rdd.take(10), tbSchema)\n",
    "\n",
    "print(titleBasics_df.show(2))\n",
    "\n",
    "#rdd to df example VELO\n",
    "\n",
    "def add_fields(line, separator):\n",
    "    fields = []\n",
    "    vals = line.split(separator)\n",
    "    # bike id\n",
    "    fields.append(int(vals[1]))\n",
    "    # day\n",
    "    if '.' in vals[9]:\n",
    "        date = datetime.strptime(vals[9][:-4], '%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        date = datetime.strptime(vals[9], '%Y-%m-%d %H:%M:%S')\n",
    "    fields.append(date.day)\n",
    "    # month\n",
    "    fields.append(date.month)\n",
    "    # weekend\n",
    "    if date.weekday() == 5 or date.weekday() == 6:\n",
    "        fields.append(1)\n",
    "    else:\n",
    "        fields.append(0)\n",
    "    # hour\n",
    "    fields.append(date.hour)\n",
    "    # minute\n",
    "    fields.append(date.minute)\n",
    "    try:\n",
    "        # station ID\n",
    "        fields.append(int(vals[4]))\n",
    "        # station data\n",
    "        sd = station_dict.value\n",
    "        e = []\n",
    "        d = sd[int(vals[4])]\n",
    "        e.append(str(d[0]))\n",
    "        e.append(int(d[1]))\n",
    "        e.append(float(d[2]))\n",
    "        e.append(float(d[3]))\n",
    "        fields.extend(e)\n",
    "    except:\n",
    "        fields.extend([-1, 'Unknown', 0, 0.0, 0.0])\n",
    "    # location length\n",
    "    try:\n",
    "        fields.append(float(vals[18]))\n",
    "    except:\n",
    "        fields.append(0.0)\n",
    "    # location type\n",
    "    fields.append(vals[8])\n",
    "    \n",
    "    return fields\n",
    "\n",
    "\n",
    "# convert to a dataframe\n",
    "def convert_to_df(month):\n",
    "    if month == '2015-01':\n",
    "        separator = '\\t'\n",
    "    else:\n",
    "        separator = ';'\n",
    "    \n",
    "    month_data = sc.textFile('../DEMO_Velo/' + month + '.csv.gz')\n",
    "    month_rdd = (month_data\n",
    "                    .filter(lambda l: 'BikeLocationID' not in l)\n",
    "                    .map(lambda l: add_fields(l, separator))\n",
    "                    .filter(lambda l: len(l) == 13))\n",
    "    \n",
    "    # apply the schema to the RDD.\n",
    "    month_df = sqlContext.createDataFrame(month_rdd, schema)\n",
    "\n",
    "    return month_df\n",
    "\n",
    "fields = []\n",
    "fields.append(StructField('bike_id', IntegerType(), True))\n",
    "fields.append(StructField('day', IntegerType(), True))\n",
    "fields.append(StructField('month', IntegerType(), True))\n",
    "fields.append(StructField('weekend', IntegerType(), True))\n",
    "fields.append(StructField('hour', IntegerType(), True))\n",
    "fields.append(StructField('minute', IntegerType(), True))\n",
    "fields.append(StructField('station_id', IntegerType(), True))\n",
    "fields.append(StructField('station_name', StringType(), True))\n",
    "fields.append(StructField('slots', IntegerType(), True))\n",
    "fields.append(StructField('lat', FloatType(), True))\n",
    "fields.append(StructField('lng', FloatType(), True))\n",
    "fields.append(StructField('location_length', FloatType(), True))\n",
    "fields.append(StructField('location_type', StringType(), True))\n",
    "schema = StructType(fields)\n",
    "\n",
    "for i in range(1, 12):\n",
    "    try:\n",
    "        month = '2015-' + str(i).zfill(2)\n",
    "        convert_to_df(month).registerTempTable(MONTHS[i - 1] + '_table')\n",
    "        sql = 'SELECT COUNT(*) AS CNT, SUM(location_length) AS SUM FROM ' + MONTHS[i - 1] + '_table WHERE location_type = \\'A\\''\n",
    "        bike_data.append(sqlContext.sql(sql).collect())\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        bike_data.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
